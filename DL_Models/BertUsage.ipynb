{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert 模型的简易使用说明\n",
    "**这段时间又仔细学习了下bert的具体使用方法，这里做个小总结，只总结用法，不涉及原理，方便日后再查询**    \n",
    "这里主要分为3部分，即：  \n",
    "1. 模型的加载（实例化）\n",
    "2. 模型保存\n",
    "3. 模型的使用：tokenization, 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import BertConfig, BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = 'Bert_cn'\n",
    "tuned_bert = 'Bert_cn/qa_tuned_bert.pth'\n",
    "model_save_path = 'usage_path/'\n",
    "tokenizer_path = 'Bert_cn/vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载预训练Bert模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 直接使用bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Bert_cn were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接加载预训练的中文bert模型\n",
    "Model = BertModel.from_pretrained(pretrained_model_path)\n",
    "# 当然也可以用自己微调好的模型替换参数\n",
    "Model.load_state_dict(torch.load(tuned_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 使用bert作为我们模型的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Bert_cn were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model init done!\n"
     ]
    }
   ],
   "source": [
    "# 定义我们自己的模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pretrained_model: str, tuned_bert: str, pooling: str):\n",
    "        super().__init__()\n",
    "        # 先初始化bert\n",
    "        self.query_bert = BertModel.from_pretrained(pretrained_model)\n",
    "        # 再加载我之前微调的模型\n",
    "        self.query_bert.load_state_dict(torch.load(tuned_bert))\n",
    "        # 其余的初始化\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, sent):\n",
    "        query_out = self.query_bert(sent[0], sent[1], sent[2], output_hidden_states=True)\n",
    "        # 取哪一部分作为句向量？\n",
    "        # cls：[CLS]这个token\n",
    "        # pooler: [CLS]这个token再过一下全连接层+Tanh激活函数，作为该句子的特征向量。\n",
    "        if self.pooling == 'cls':\n",
    "            out = query_out[0][:, 0]  # [batch_size, 768]\n",
    "        if self.pooling == 'pooler':\n",
    "            out = query_out.pooler_output  # [batch_size, 768]\n",
    "        return out\n",
    "\n",
    "# 实例模型\n",
    "Model = MyModel(pretrained_model_path, tuned_bert, 'cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型的保存\n",
    "### 2.1 保存模型参数（推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model.state_dict(), model_save_path+'model_1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 保存整个模型（要占用内存，不推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model, model_save_path+'model_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 保存模型的某一个部分\n",
    "**例如这里我们保存之前MyModel里的‘query_bert’这一部分的参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model.query_bert.state_dict(), model_save_path+'model_3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 模型的使用\n",
    "### 3.1 tokenization\n",
    "**首先，需要实例tokenizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1706: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**然后处理我们的单个句子:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  872, 1962, 8024, 2769, 3221, 3412, 4001,  102,    0,    0,    0,\n",
      "            0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "sent = '你好，我是栗滕'\n",
    "sent_source = tokenizer(sent,\n",
    "                        max_length=15,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt')\n",
    "# 得到bert模型需要的输入\n",
    "input_ids = sent_source['input_ids']               # input_ids 是句子token化，id化之后的结果\n",
    "attention_mask = sent_source['attention_mask']     # attention_mask 是mask标记，1表示没有被遮盖，0表示被遮盖（padding的数据会被遮盖）\n",
    "token_type_ids = sent_source['token_type_ids']     # token_type_ids 是token类型标记，0表示第一句话，1表示第二句\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当然tokenizer还可以处理两个句子:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  872, 1962, 7357, 3926, 2813,  102, 2769, 3221, 4374,  753,  102,\n",
      "            0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "sent1 = '你好陈清扬'\n",
    "sent2 = '我是王二'\n",
    "sent_source = tokenizer(sent1, sent2,\n",
    "                        max_length=15,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt')\n",
    "# 得到bert模型需要的输入\n",
    "input_ids = sent_source['input_ids']               # input_ids 是句子token化，id化之后的结果，101是[CLS]，102是[SEP]\n",
    "attention_mask = sent_source['attention_mask']     # attention_mask 是mask标记，1表示没有被遮盖，0表示被遮盖（padding的数据会被遮盖）\n",
    "token_type_ids = sent_source['token_type_ids']     # token_type_ids 是token类型标记，0表示第一句话，1表示第二句\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tokenizer输入也可以是一个list的句子，你可以理解为一个batch的句子:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  872, 1962, 7357, 3926, 2813,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2769, 3221, 4374,  753,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 6702,  836, 1920, 4638, 1351, 6449, 8013,  102,    0,    0,    0,\n",
      "            0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "sent_list = ['你好陈清扬', '我是王二', '蹲伟大的友谊！']\n",
    "sent_source = tokenizer(sent_list,\n",
    "                        max_length=15,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt')\n",
    "# 得到bert模型需要的输入\n",
    "input_ids = sent_source['input_ids']               # input_ids 是句子token化，id化之后的结果，101是[CLS]，102是[SEP]\n",
    "attention_mask = sent_source['attention_mask']     # attention_mask 是mask标记，1表示没有被遮盖，0表示被遮盖（padding的数据会被遮盖）\n",
    "token_type_ids = sent_source['token_type_ids']     # token_type_ids 是token类型标记，0表示第一句话，1表示第二句\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 forward\n",
    "**首先前向传播，得到模型输出(Out)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out = Model(input_ids, attention_mask, token_type_ids, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**然后我们来看看模型的输出都有哪些，以及对应的格式    \n",
    "首先看输出的种类**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "print(Out.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以发现，模型有3种，分别是last_hidden_state， pooler_output， hidden_states    \n",
    "last_hidden_state：最后一层输出，形状是[batch_size, seq_lenth, 768]    \n",
    "pooler: 常用来表示句向量本质上是‘[CLS]’这个token的tensor再过一下全连接层+Tanh激活函数，形状是 [batch_size, 768]    \n",
    "hidden_states: 13层每层的输出，tuple，长度为13，每一层形状[batch_size, seq_lenth, 768]，它的最后一层也就是last_hidden_state    \n",
    "而我们常常使用的cls，则是last_hidden_state的第一个向量，形状[batch_size, 768]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state_out形状: torch.Size([3, 15, 768])\n",
      "pooler_out形状: torch.Size([3, 768])\n",
      "bert隐藏层层数: 13\n",
      "每层shape: torch.Size([3, 15, 768])\n",
      "最后一层[-1]是否等于last_hidden_state: tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "cls的形状： torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "# last_hidden_state\n",
    "last_hidden_state_out = Out['last_hidden_state']\n",
    "print('last_hidden_state_out形状:', last_hidden_state_out.shape)\n",
    "# pooler\n",
    "pooler_out = Out['pooler_output']\n",
    "print('pooler_out形状:', pooler_out.shape)\n",
    "# hidden_states, \n",
    "hidden_states_out = Out['hidden_states']\n",
    "print('bert隐藏层层数:',len(hidden_states_out))\n",
    "print('每层shape:',hidden_states_out[0].shape)\n",
    "print('最后一层[-1]是否等于last_hidden_state:', hidden_states_out[-1]==last_hidden_state_out)\n",
    "# cls\n",
    "cls_out = Out['last_hidden_state'][:,0]\n",
    "print('cls的形状：', cls_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**同时我们可以知道，Out['last_hidden_state'] 和 Out[0] 是同一个数据 (同理，Out[1]代表pooler，Out[2]是hideen_states)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(Out['last_hidden_state'] == Out[0])\n",
    "print(Out['pooler_output'] == Out[1])\n",
    "print(Out['hidden_states'] == Out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda:base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
