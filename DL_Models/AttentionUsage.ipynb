{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention核心思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们准备一些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2328, 0.1955, 0.4514, 0.3852, 0.4502, 0.1747, 0.1463, 0.0992,\n",
      "          0.4605, 0.2810],\n",
      "         [0.3783, 0.4232, 0.1628, 0.2301, 0.2317, 0.3193, 0.4110, 0.1351,\n",
      "          0.1864, 0.4699],\n",
      "         [0.4324, 0.4321, 0.1258, 0.3348, 0.3773, 0.2643, 0.0188, 0.4271,\n",
      "          0.3183, 0.0460],\n",
      "         [0.0199, 0.4182, 0.1126, 0.6190, 0.1190, 0.0155, 0.3584, 0.3132,\n",
      "          0.4240, 0.0905]],\n",
      "\n",
      "        [[0.2952, 0.0060, 0.4770, 0.0710, 0.1280, 0.2444, 0.2332, 0.1486,\n",
      "          0.7071, 0.1662],\n",
      "         [0.0251, 0.1355, 0.2461, 0.2914, 0.1460, 0.1670, 0.6920, 0.3603,\n",
      "          0.3921, 0.1545],\n",
      "         [0.1063, 0.2384, 0.7215, 0.1637, 0.1687, 0.2931, 0.3835, 0.0703,\n",
      "          0.1135, 0.3243],\n",
      "         [0.4006, 0.4957, 0.0089, 0.1899, 0.1000, 0.1672, 0.1317, 0.4992,\n",
      "          0.0395, 0.5016]]])\n"
     ]
    }
   ],
   "source": [
    "# 先假设一些参数\n",
    "vec_size = 10\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "# 初始化一些数据\n",
    "sents = torch.rand(batch_size,seq_len,vec_size)\n",
    "# 这里我们都使用单位向量，这样结果比较明显\n",
    "sents = f.normalize(sents, p=2, dim=2)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.这里先尝试使用简单的单头self_attention来解释attention的思想。\n",
    "attention的核心思想，简单的说就是，先用q和k对比，求一个相似性，然后根据这个相似性，按照比例提取v，组成新的embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, vec_size):\n",
    "        super().__init__()\n",
    "        # （Q，K，V）3个w矩阵\n",
    "        self.QueryW = nn.Linear(vec_size, vec_size)\n",
    "        self.KeyW = nn.Linear(vec_size, vec_size)\n",
    "        self.ValueW = nn.Linear(vec_size, vec_size)\n",
    "        self.Score2Prob = nn.Softmax(dim=-1)\n",
    "            \n",
    "    def forward(self, sents):\n",
    "        '''\n",
    "        sents: [batch_size, seq_len, vec_size]\n",
    "        '''\n",
    "        print('sents.shape: ', sents.shape)\n",
    "        # 先计算q，k，v向量\n",
    "        Q = self.QueryW(sents)  #[batch_size, seq_len, vec_size]\n",
    "        K = self.KeyW(sents)    #[batch_size, seq_len, vec_size]\n",
    "        V = self.ValueW(sents)  #[batch_size, seq_len, vec_size]\n",
    "        # 再计算q和k的相似值，并转成概率分布\n",
    "        Scores = torch.matmul(Q, K.transpose(1,2).contiguous())  #[batch_size, seq_len, seq_len]\n",
    "        Scores = self.Score2Prob(Scores)  #[batch_size, seq_len, seq_len]\n",
    "        # 再输出新的embedding，即：相似值*V\n",
    "        Out = torch.matmul(Scores,V)  #[batch_size, seq_len, vec_size]\n",
    "        return Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents.shape:  torch.Size([2, 4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0109,  0.1471,  0.1141, -0.3939, -0.1368, -0.3375, -0.3938,\n",
       "          -0.1718, -0.0318, -0.1074],\n",
       "         [ 0.0109,  0.1472,  0.1141, -0.3939, -0.1368, -0.3374, -0.3938,\n",
       "          -0.1718, -0.0318, -0.1073],\n",
       "         [ 0.0095,  0.1473,  0.1144, -0.3940, -0.1366, -0.3370, -0.3942,\n",
       "          -0.1720, -0.0334, -0.1068],\n",
       "         [ 0.0105,  0.1461,  0.1141, -0.3941, -0.1360, -0.3375, -0.3934,\n",
       "          -0.1725, -0.0324, -0.1070]],\n",
       "\n",
       "        [[-0.0121,  0.1051,  0.1865, -0.3757, -0.1369, -0.2970, -0.3399,\n",
       "          -0.2815, -0.1429, -0.2604],\n",
       "         [-0.0124,  0.1052,  0.1873, -0.3763, -0.1371, -0.2966, -0.3395,\n",
       "          -0.2797, -0.1408, -0.2595],\n",
       "         [-0.0176,  0.1046,  0.1850, -0.3746, -0.1357, -0.2978, -0.3419,\n",
       "          -0.2856, -0.1517, -0.2622],\n",
       "         [-0.0201,  0.1046,  0.1852, -0.3750, -0.1354, -0.2980, -0.3426,\n",
       "          -0.2861, -0.1540, -0.2625]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SelfAttention(vec_size=vec_size)\n",
    "model(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.接下来我们尝试自己实现一下Multi-head Attention。\n",
    "和单头相比，多头就是多个单头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, vec_size, head_num):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.vec_size = vec_size\n",
    "        self.QueryW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.KeyW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.ValueW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.Score2Prob = nn.Softmax(dim=2)\n",
    "        self.OutW = nn.Linear(vec_size*head_num, vec_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('sents.shape: ', sents.shape)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        vec_size = self.vec_size\n",
    "        head_num = self.head_num\n",
    "        # 同样先计算q，k，v向量，但这次是多头。\n",
    "        Q = self.QueryW(x).view(batch_size, seq_len, head_num, vec_size)\n",
    "        K = self.KeyW(x).view(batch_size, seq_len, head_num, vec_size)\n",
    "        V = self.ValueW(x).view(batch_size, seq_len, head_num, vec_size)\n",
    "        # 再计算q和k的相似值，并转成概率分布\n",
    "        Q = Q.transpose(1,2).contiguous().view(batch_size*head_num, seq_len, vec_size)  # [batch_size*head_num, seq_len, vec_size]\n",
    "        K = K.transpose(1,2).contiguous().view(batch_size*head_num, seq_len, vec_size)\n",
    "        V = V.transpose(1,2).contiguous().view(batch_size*head_num, seq_len, vec_size)\n",
    "        Score = torch.matmul(Q, K.transpose(1,2).contiguous())\n",
    "        Score = Score/np.sqrt(vec_size)\n",
    "        Score = self.Score2Prob(Score)  # [batch_size*head_num, seq_len, seq_len]\n",
    "        # 再输出新的embedding，即：相似值*V\n",
    "        Out = torch.matmul(Score,V)  # [batch_size*head_num, seq_len, vec_size]\n",
    "        Out = Out.view(batch_size, head_num, seq_len, vec_size)  # [batch_size, head_num, seq_len, vec_size]\n",
    "        # 将多头分别得到的新embed，转为一个embed\n",
    "        Out = Out.transpose(1,2).contiguous().view(batch_size, seq_len, head_num*vec_size)  # [batch_size, seq_len, head_num*vec_size]\n",
    "        Out = self.OutW(Out)\n",
    "        return Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents.shape:  torch.Size([2, 4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1450,  0.2247, -0.0268, -0.0157, -0.0866, -0.3739,  0.2823,\n",
       "          -0.1348,  0.0882,  0.0622],\n",
       "         [-0.1447,  0.2250, -0.0263, -0.0158, -0.0873, -0.3738,  0.2825,\n",
       "          -0.1345,  0.0883,  0.0619],\n",
       "         [-0.1448,  0.2250, -0.0264, -0.0158, -0.0871, -0.3737,  0.2823,\n",
       "          -0.1346,  0.0884,  0.0620],\n",
       "         [-0.1447,  0.2248, -0.0263, -0.0156, -0.0872, -0.3739,  0.2824,\n",
       "          -0.1351,  0.0881,  0.0623]],\n",
       "\n",
       "        [[-0.1431,  0.2354,  0.0481,  0.0087, -0.1031, -0.3085,  0.2405,\n",
       "          -0.1824,  0.1316,  0.0426],\n",
       "         [-0.1426,  0.2348,  0.0483,  0.0087, -0.1037, -0.3087,  0.2404,\n",
       "          -0.1829,  0.1314,  0.0421],\n",
       "         [-0.1423,  0.2355,  0.0494,  0.0084, -0.1030, -0.3088,  0.2410,\n",
       "          -0.1816,  0.1320,  0.0417],\n",
       "         [-0.1413,  0.2345,  0.0498,  0.0087, -0.1033, -0.3098,  0.2417,\n",
       "          -0.1814,  0.1326,  0.0405]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyMultiHeadAttention(vec_size=vec_size, head_num=3)\n",
    "model(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.接下来我们尝试使用pytorch的Multi-head Attention。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, vec_size, head_num):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.vec_size = vec_size\n",
    "        self.QueryW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.KeyW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.ValueW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.Multihead_attn = nn.MultiheadAttention(vec_size*head_num, head_num, batch_first=True)\n",
    "        self.OutW = nn.Linear(vec_size*head_num, vec_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print('sents.shape: ', sents.shape)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        vec_size = self.vec_size\n",
    "        head_num = self.head_num\n",
    "        # 同样先计算q，k，v向量，但这次是多头。\n",
    "        Q = self.QueryW(x)  #[batch_size, seq_len, vec_size]\n",
    "        K = self.KeyW(x)\n",
    "        V = self.ValueW(x)\n",
    "        # mutihead attention\n",
    "        Out, Weights = self.Multihead_attn(Q, K, V)\n",
    "        # 将多头分别得到的新embed，转为一个embed\n",
    "        Out = self.OutW(Out)\n",
    "        print(Out.shape)\n",
    "        return Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents.shape:  torch.Size([2, 4, 10])\n",
      "torch.Size([2, 4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1454, -0.0547,  0.0199, -0.2036,  0.2947,  0.0182,  0.0481,\n",
       "          -0.0444,  0.0834, -0.2219],\n",
       "         [ 0.1454, -0.0547,  0.0198, -0.2037,  0.2947,  0.0182,  0.0481,\n",
       "          -0.0444,  0.0834, -0.2218],\n",
       "         [ 0.1454, -0.0547,  0.0199, -0.2036,  0.2947,  0.0182,  0.0481,\n",
       "          -0.0444,  0.0833, -0.2219],\n",
       "         [ 0.1454, -0.0545,  0.0199, -0.2037,  0.2946,  0.0184,  0.0480,\n",
       "          -0.0444,  0.0834, -0.2217]],\n",
       "\n",
       "        [[ 0.1523, -0.0366, -0.0088, -0.1952,  0.2638,  0.0124,  0.0537,\n",
       "          -0.0112,  0.1072, -0.2007],\n",
       "         [ 0.1523, -0.0361, -0.0088, -0.1954,  0.2636,  0.0130,  0.0536,\n",
       "          -0.0112,  0.1068, -0.2004],\n",
       "         [ 0.1522, -0.0364, -0.0087, -0.1953,  0.2638,  0.0126,  0.0537,\n",
       "          -0.0112,  0.1071, -0.2005],\n",
       "         [ 0.1523, -0.0362, -0.0087, -0.1954,  0.2637,  0.0129,  0.0537,\n",
       "          -0.0113,  0.1068, -0.2005]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadAttention(vec_size=vec_size, head_num=3)\n",
    "model(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda:base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
