{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention核心思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们准备一些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1126, 0.0947, 0.2096, 0.3071, 0.4642, 0.0592, 0.2444, 0.3948,\n",
      "          0.4472, 0.4533],\n",
      "         [0.0581, 0.4792, 0.1842, 0.1569, 0.1049, 0.3199, 0.4926, 0.2041,\n",
      "          0.2906, 0.4759],\n",
      "         [0.0434, 0.3689, 0.5747, 0.2188, 0.0666, 0.0442, 0.3289, 0.4632,\n",
      "          0.3774, 0.1112],\n",
      "         [0.5800, 0.2434, 0.2349, 0.2005, 0.3672, 0.0239, 0.2227, 0.0588,\n",
      "          0.3605, 0.4366]],\n",
      "\n",
      "        [[0.2391, 0.3953, 0.0980, 0.2839, 0.1098, 0.3597, 0.3866, 0.3801,\n",
      "          0.4072, 0.3085],\n",
      "         [0.0019, 0.1059, 0.1363, 0.2362, 0.3724, 0.2176, 0.3953, 0.2270,\n",
      "          0.7115, 0.1198],\n",
      "         [0.3452, 0.4973, 0.2242, 0.4729, 0.2477, 0.2316, 0.4679, 0.0854,\n",
      "          0.0620, 0.1210],\n",
      "         [0.0453, 0.2274, 0.1182, 0.3428, 0.3818, 0.2016, 0.2029, 0.4490,\n",
      "          0.4221, 0.4554]]])\n"
     ]
    }
   ],
   "source": [
    "# 先假设一些参数\n",
    "vec_size = 10\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "# 初始化一些数据\n",
    "sents = torch.rand(batch_size,seq_len,vec_size)\n",
    "# 这里我们都使用单位向量，这样结果比较明显\n",
    "sents = f.normalize(sents, p=2, dim=2)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.这里先尝试使用简单的单头self_attention来解释attention的思想。\n",
    "attention的核心思想，简单的说就是，先用q和k对比，求一个相似性，然后根据这个相似性，按照比例提取v，组成新的embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, vec_size):\n",
    "        super().__init__()\n",
    "        # （Q，K，V）3个w矩阵\n",
    "        self.QueryW = nn.Linear(vec_size, vec_size)\n",
    "        self.KeyW = nn.Linear(vec_size, vec_size)\n",
    "        self.ValueW = nn.Linear(vec_size, vec_size)\n",
    "        self.Score2Prob = nn.Softmax(dim=-1)\n",
    "            \n",
    "    def forward(self, sents):\n",
    "        '''\n",
    "        sents: [batch_size, seq_len, vec_size]\n",
    "        '''\n",
    "        print('sents.shape: ', sents.shape)\n",
    "        # 先计算q，k，v向量\n",
    "        Q = self.QueryW(sents)  #[batch_size, seq_len, vec_size]\n",
    "        K = self.KeyW(sents)    #[batch_size, seq_len, vec_size]\n",
    "        V = self.ValueW(sents)  #[batch_size, seq_len, vec_size]\n",
    "        # 再计算q和k的相似值，并转成概率分布\n",
    "        Scores = torch.matmul(Q, K.transpose(1,2).contiguous())  #[batch_size, seq_len, seq_len]\n",
    "        Scores = self.Score2Prob(Scores)  #[batch_size, seq_len, seq_len]\n",
    "        # 再输出新的embedding，即：相似值*V\n",
    "        Out = torch.matmul(Scores,V)  #[batch_size, seq_len, vec_size]\n",
    "        return Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents.shape:  torch.Size([2, 4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0399,  0.0413,  0.2227,  0.0572,  0.1893, -0.1025, -0.2460,\n",
       "           0.0029, -0.2069, -0.5122],\n",
       "         [-0.0416,  0.0439,  0.2222,  0.0568,  0.1875, -0.1060, -0.2449,\n",
       "           0.0014, -0.2077, -0.5153],\n",
       "         [-0.0428,  0.0429,  0.2235,  0.0567,  0.1883, -0.1074, -0.2472,\n",
       "           0.0032, -0.2050, -0.5153],\n",
       "         [-0.0387,  0.0411,  0.2214,  0.0565,  0.1897, -0.1001, -0.2447,\n",
       "           0.0017, -0.2089, -0.5110]],\n",
       "\n",
       "        [[-0.1623,  0.0629,  0.1961, -0.0138,  0.2406, -0.1183, -0.2788,\n",
       "          -0.0576, -0.1563, -0.4873],\n",
       "         [-0.1616,  0.0633,  0.1948, -0.0140,  0.2412, -0.1179, -0.2779,\n",
       "          -0.0574, -0.1575, -0.4875],\n",
       "         [-0.1626,  0.0630,  0.1964, -0.0141,  0.2405, -0.1184, -0.2791,\n",
       "          -0.0575, -0.1563, -0.4873],\n",
       "         [-0.1623,  0.0631,  0.1963, -0.0133,  0.2408, -0.1190, -0.2786,\n",
       "          -0.0577, -0.1558, -0.4876]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SelfAttention(vec_size=vec_size)\n",
    "model(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.接下来我们尝试Multi-head Attention。\n",
    "和单头相比，多头就是多个单头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, vec_size, head_num):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.vec_size = vec_size\n",
    "        self.QueryW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.KeyW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.ValueW = nn.Linear(vec_size, vec_size*head_num)\n",
    "        self.Score2Prob = nn.Softmax(dim=2)\n",
    "        self.OutW = nn.Linear(vec_size*head_num, vec_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('sents.shape: ', sents.shape)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        vec_size = self.vec_size\n",
    "        head_num = self.head_num\n",
    "        # 同样先计算q，k，v向量，但这次是多头。\n",
    "        Q = self.QueryW(x).view(batch_size, seq_len, head_num, vec_size)\n",
    "        K = self.KeyW(x).view(batch_size, seq_len, head_num, vec_size)\n",
    "        V = self.ValueW(x).view(batch_size, seq_len, head_num, vec_size)\n",
    "        # 再计算q和k的相似值，并转成概率分布\n",
    "        Q = Q.transpose(1,2).contiguous().view(batch_size*head_num, seq_len, vec_size)  # [batch_size*head_num, seq_len, vec_size]\n",
    "        K = K.transpose(1,2).contiguous().view(batch_size*head_num, seq_len, vec_size)\n",
    "        V = V.transpose(1,2).contiguous().view(batch_size*head_num, seq_len, vec_size)\n",
    "        Score = torch.matmul(Q, K.transpose(1,2).contiguous())\n",
    "        Score = Score/np.sqrt(vec_size)\n",
    "        Score = self.Score2Prob(Score)  # [batch_size*head_num, seq_len, seq_len]\n",
    "        # 再输出新的embedding，即：相似值*V\n",
    "        Out = torch.matmul(Score,V)  # [batch_size*head_num, seq_len, vec_size]\n",
    "        Out = Out.view(batch_size, head_num, seq_len, vec_size)  # [batch_size, head_num, seq_len, vec_size]\n",
    "        # 将多头分别得到的新embed，转为一个embed\n",
    "        Out = Out.transpose(1,2).contiguous().view(batch_size, seq_len, head_num*vec_size)  # [batch_size, seq_len, head_num*vec_size]\n",
    "        Out = self.OutW(Out)\n",
    "        return Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents.shape:  torch.Size([2, 4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1886, -0.3139,  0.0119, -0.4636, -0.1918,  0.0862,  0.0148,\n",
       "          -0.3379, -0.0632,  0.2887],\n",
       "         [ 0.1890, -0.3137,  0.0117, -0.4632, -0.1927,  0.0860,  0.0143,\n",
       "          -0.3378, -0.0633,  0.2893],\n",
       "         [ 0.1888, -0.3136,  0.0120, -0.4637, -0.1925,  0.0863,  0.0144,\n",
       "          -0.3378, -0.0633,  0.2892],\n",
       "         [ 0.1890, -0.3136,  0.0120, -0.4639, -0.1925,  0.0863,  0.0143,\n",
       "          -0.3378, -0.0628,  0.2891]],\n",
       "\n",
       "        [[ 0.1472, -0.3800, -0.0130, -0.4557, -0.2222,  0.0819, -0.0080,\n",
       "          -0.3445, -0.0653,  0.2745],\n",
       "         [ 0.1473, -0.3795, -0.0122, -0.4558, -0.2221,  0.0819, -0.0080,\n",
       "          -0.3437, -0.0651,  0.2745],\n",
       "         [ 0.1473, -0.3800, -0.0133, -0.4558, -0.2224,  0.0822, -0.0079,\n",
       "          -0.3446, -0.0656,  0.2748],\n",
       "         [ 0.1472, -0.3798, -0.0129, -0.4557, -0.2221,  0.0818, -0.0079,\n",
       "          -0.3443, -0.0651,  0.2745]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadAttention(vec_size=vec_size, head_num=3)\n",
    "model(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda:base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
